{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Dense, Dropout, Embedding\n",
    "\n",
    "\n",
    "# GPT only uses the decoder part of the Transformer architecture\n",
    "# We can remove the Encoder and EncoderLayer classes\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = tf.range(position, dtype=tf.float32)[:, tf.newaxis] * 1 / tf.pow(10000, (2 * tf.range(0, d_model, dtype=tf.float32)) / d_model)\n",
    "    angle_rads_even = tf.math.sin(angle_rads[:, 0::2])\n",
    "    angle_rads_odd = tf.math.cos(angle_rads[:, 1::2])\n",
    "    angle_rads = tf.stack([angle_rads_even, angle_rads_odd], axis=-1)\n",
    "    angle_rads = tf.reshape(angle_rads, (-1, d_model))\n",
    "    pos_encoding = angle_rads[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Decoder layer\n",
    "class GPTLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.15):\n",
    "        super(GPTLayer, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.mha._build_from_signature(input_shape, input_shape, input_shape)\n",
    "        super(GPTLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    if size == 1:\n",
    "        return tf.zeros((1, 1))\n",
    "    else:\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "# GPT model\n",
    "# TODO do I need to mask padding?\n",
    "class GPT(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_position_encoding, rate=0.15):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_position_encoding, d_model)\n",
    "\n",
    "        self.gpt_layers = [GPTLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "        self.final_layer = Dense(vocab_size)\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(inp)[1])\n",
    "        combined_mask = tf.maximum(look_ahead_mask, padding_mask)\n",
    "        return combined_mask\n",
    "\n",
    "\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        mask = self.create_masks(x)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.gpt_layers[i](x, training, mask)\n",
    "\n",
    "        final_output = self.final_layer(x)\n",
    "        last_position_logits = final_output[:, -1, :]\n",
    "        return last_position_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.12.0\n",
      "Connecting to TPU...\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Number of accelerators:  8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Embedding, Layer\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"Connecting to TPU...\")\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='node-8',zone='us-central1-f')\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "print(\"Done!\")\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the GPT2 tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "# use the gpt2 tokenizer\n",
    "tokenizer: GPT2Tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "with strategy.scope():\n",
    "    # Create the GPT model using the provided parameters\n",
    "    num_layers = 6\n",
    "    d_model = 256\n",
    "    num_heads = 8\n",
    "    dff = 1024\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    max_position_encoding = 16\n",
    "    dropout_rate = 0.2\n",
    "    learning_rate = 5e-5\n",
    "    batch_size = 8\n",
    "    epochs = 3\n",
    "    warmup_steps = 200\n",
    "    model = GPT(num_layers, d_model, num_heads, dff, vocab_size, max_position_encoding, dropout_rate)\n",
    "\n",
    "    # Define the optimizer, loss function, and metric\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    metric = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss_object, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[    0,     0,     0, ..., 24720,   286, 20153],\n",
      "       [    0,     0,     0, ...,   262, 17520,  8270],\n",
      "       [    0,     0,     0, ...,  5838,   286, 36782],\n",
      "       ...,\n",
      "       [    0,     0,     0, ...,   262,  3427,  4479],\n",
      "       [    0,     0,  2061, ...,    42,    13, 41558],\n",
      "       [    0,     0,     0, ...,  1605,  7511,  1810]], dtype=int32), array([[1776],\n",
      "       [1789],\n",
      "       [1815],\n",
      "       [1865],\n",
      "       [1876],\n",
      "       [1903],\n",
      "       [1912],\n",
      "       [1914],\n",
      "       [1917],\n",
      "       [1918],\n",
      "       [1929],\n",
      "       [1939],\n",
      "       [1941],\n",
      "       [1944],\n",
      "       [1945],\n",
      "       [1945],\n",
      "       [1945],\n",
      "       [1950],\n",
      "       [1957],\n",
      "       [1962],\n",
      "       [1963],\n",
      "       [1969],\n",
      "       [1975],\n",
      "       [1989],\n",
      "       [1991],\n",
      "       [2001],\n",
      "       [2003],\n",
      "       [1989],\n",
      "       [1968],\n",
      "       [1953],\n",
      "       [1967],\n",
      "       [1986],\n",
      "       [1990],\n",
      "       [1994],\n",
      "       [1995],\n",
      "       [1996],\n",
      "       [1997],\n",
      "       [1999],\n",
      "       [2004],\n",
      "       [2013],\n",
      "       [2016],\n",
      "       [2016],\n",
      "       [2007],\n",
      "       [2016],\n",
      "       [2003],\n",
      "       [1948],\n",
      "       [1914],\n",
      "       [1848],\n",
      "       [1914],\n",
      "       [1928],\n",
      "       [1955],\n",
      "       [1948],\n",
      "       [1993],\n",
      "       [1997],\n",
      "       [1861]], dtype=int32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Executor.__del__ at 0x7f1c68398700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/tensorflow/python/eager/executor.py\", line 46, in __del__\n",
      "    self.wait()\n",
      "  File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/tensorflow/python/eager/executor.py\", line 65, in wait\n",
      "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
      "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"What year was the signing of the Declaration of Independence? 1776\",\n",
    "\"What year was the storming of the Bastille? 1789\",\n",
    "\"What year was the Battle of Waterloo? 1815\",\n",
    "\"What year was the assassination of Abraham Lincoln? 1865\",\n",
    "\"What year was the invention of the telephone by Alexander Graham Bell? 1876\",\n",
    "\"What year was the first successful powered airplane flight by the Wright brothers? 1903\",\n",
    "\"What year was the sinking of the Titanic? 1912\",\n",
    "\"What year was the beginning of World War I? 1914\",\n",
    "\"What year was the Russian Revolution? 1917\",\n",
    "\"What year was the end of World War I? 1918\",\n",
    "\"What year was the stock market crash that led to the Great Depression? 1929\",\n",
    "\"What year was the beginning of World War II? 1939\",\n",
    "\"What year was the attack on Pearl Harbor? 1941\",\n",
    "\"What year was the D-Day invasion during World War II? 1944\",\n",
    "\"What year was the dropping of the atomic bombs on Hiroshima and Nagasaki? 1945\",\n",
    "\"What year was the end of World War II? 1945\",\n",
    "\"What year was the establishment of the United Nations? 1945\",\n",
    "\"What year was the beginning of the Korean War? 1950\",\n",
    "\"What year was the launch of Sputnik 1, the first artificial satellite? 1957\",\n",
    "\"What year was the Cuban Missile Crisis? 1962\",\n",
    "\"What year was the assassination of John F. Kennedy? 1963\",\n",
    "\"What year was the first moon landing by Apollo 11? 1969\",\n",
    "\"What year was the end of the Vietnam War? 1975\",\n",
    "\"What year was the fall of the Berlin Wall? 1989\",\n",
    "\"What year was the dissolution of the Soviet Union? 1991\",\n",
    "\"What year was the terrorist attacks on September 11? 2001\",\n",
    "\"What year was the beginning of the Iraq War? 2003\",\n",
    "\"What year was the invention of the World Wide Web by Tim Berners-Lee? 1989\",\n",
    "\"What year was the assassination of Martin Luther King Jr.? 1968\",\n",
    "\"What year was the discovery of DNA's double helix structure by James Watson and Francis Crick? 1953\",\n",
    "\"What year was the first human heart transplant performed by Dr. Christiaan Barnard? 1967\",\n",
    "\"What year was the Chernobyl nuclear disaster? 1986\",\n",
    "\"What year was the launch of the Hubble Space Telescope? 1990\",\n",
    "\"What year was the Rwandan Genocide? 1994\",\n",
    "\"What year was the Oklahoma City bombing? 1995\",\n",
    "\"What year was the cloning of Dolly the sheep? 1996\",\n",
    "\"What year was the death of Princess Diana? 1997\",\n",
    "\"What year was the Euro currency introduced? 1999\",\n",
    "\"What year was the Indian Ocean earthquake and tsunami? 2004\",\n",
    "\"What year was the election of Pope Francis? 2013\",\n",
    "\"What year was the Paris Agreement on climate change signed? 2016\",\n",
    "\"What year was the Brexit referendum? 2016\",\n",
    "\"What year was the first iPhone released? 2007\",\n",
    "\"What year was the election of Donald Trump as the 45th President of the United States? 2016\",\n",
    "\"What year was the completion of the Human Genome Project? 2003\",\n",
    "\"What year was the founding of the World Health Organization? 1948\",\n",
    "\"What year was the assassination of Archduke Franz Ferdinand? 1914\",\n",
    "\"What year was the start of the California Gold Rush? 1848\",\n",
    "\"What year was the completion of the Panama Canal? 1914\",\n",
    "\"What year was the discovery of penicillin by Alexander Fleming? 1928\",\n",
    "\"What year was the Montgomery Bus Boycott? 1955\",\n",
    "\"What year was the assassination of Mahatma Gandhi? 1948\",\n",
    "\"What year was the formation of the European Union? 1993\",\n",
    "\"What year was the release of the first Harry Potter book by J.K. Rowling? 1997\",\n",
    "\"What year was the start of the American Civil War? 1861\"]\n",
    "\n",
    "def create_tf_dataset(data, tokenizer):\n",
    "    def split_input_target(input_string):\n",
    "        parts = input_string.strip().split(\"? \")\n",
    "        event, year = \" \".join(parts[:-1]), int(parts[-1])\n",
    "        return event, year\n",
    "\n",
    "    events, years = zip(*[split_input_target(item) for item in data])\n",
    "    \n",
    "    # Encode events using GPT-2 tokenizer\n",
    "    encoded_events = [tokenizer.encode(event) for event in events]\n",
    "    \n",
    "    events_max_length = max([len(event) for event in encoded_events])\n",
    "    \n",
    "    encoded_events = [[0] * (events_max_length - len(event)) + event for event in encoded_events]\n",
    "    \n",
    "    encoded_years = [int(year) for year in years]\n",
    "    encoded_years = tf.expand_dims(encoded_years, -1)\n",
    "    events_tensor = tf.data.Dataset.from_tensor_slices([encoded_events])\n",
    "    years_tensor = tf.data.Dataset.from_tensor_slices([encoded_years])\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((events_tensor, years_tensor))\n",
    "\n",
    "    return dataset\n",
    "dataset = create_tf_dataset(data, tokenizer)\n",
    "[print(item) for item in dataset.as_numpy_iterator()]\n",
    "# dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n        outputs = model.train_step(data)\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filepe0tk9ch.py\", line 16, in tf__call\n        x += self.pos_encoding[:, :seq_len, :]\n\n    ValueError: Exception encountered when calling layer 'gpt' (type GPT).\n    \n    in user code:\n    \n        File \"<ipython-input-3-0312b98c80e0>\", line 87, in call  *\n            x += self.pos_encoding[:, :seq_len, :]\n    \n        ValueError: Dimensions must be equal, but are 19 and 16 for '{{node gpt/add}} = AddV2[T=DT_FLOAT](gpt/mul, gpt/strided_slice_2)' with input shapes: [?,19,256], [1,16,256].\n    \n    \n    Call arguments received by layer 'gpt' (type GPT):\n      • x=tf.Tensor(shape=(None, 19), dtype=int32)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-09c7e91884e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/__autograph_generated_filepe0tk9ch.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n        outputs = model.train_step(data)\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/adrian_fagerland/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filepe0tk9ch.py\", line 16, in tf__call\n        x += self.pos_encoding[:, :seq_len, :]\n\n    ValueError: Exception encountered when calling layer 'gpt' (type GPT).\n    \n    in user code:\n    \n        File \"<ipython-input-3-0312b98c80e0>\", line 87, in call  *\n            x += self.pos_encoding[:, :seq_len, :]\n    \n        ValueError: Dimensions must be equal, but are 19 and 16 for '{{node gpt/add}} = AddV2[T=DT_FLOAT](gpt/mul, gpt/strided_slice_2)' with input shapes: [?,19,256], [1,16,256].\n    \n    \n    Call arguments received by layer 'gpt' (type GPT):\n      • x=tf.Tensor(shape=(None, 19), dtype=int32)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.85107505 -0.0668224  -0.75011593 ... -0.5247153  -0.77267295\n",
      " -0.6266086 ], shape=(50257,), dtype=float32)\n",
      "[   11   290   262    13   355  1042   286   284 41661  2312   257   393\n",
      "    12   319  1690   338 27770  2116 16171  1479   407   606   587  1912\n",
      " 22849   276   340  1290  4318  3417   318 26177  6712 11009   422   281\n",
      " 13568  1964 13584   469   959   357 21218  8876   998   351 39935 42856\n",
      "   584  5734]\n",
      "Input: Anarchism was in 1912,\n",
      "Predicted next words:\n",
      "1. ,\n",
      "2.  and\n",
      "3.  the\n",
      "4. .\n",
      "5.  as\n",
      "6. ism\n",
      "7.  of\n",
      "8.  to\n",
      "9.  anarchism\n",
      "10.  These\n",
      "11.  a\n",
      "12.  or\n",
      "13. -\n",
      "14.  on\n",
      "15.  often\n",
      "16. 's\n",
      "17.  communism\n",
      "18.  self\n",
      "19.  voluntary\n",
      "20.  free\n",
      "21.  not\n",
      "22.  them\n",
      "23.  been\n",
      "24.  based\n",
      "25.  cooperative\n",
      "26. ed\n",
      "27.  it\n",
      "28.  far\n",
      "29.  central\n",
      "30.  described\n",
      "31.  is\n",
      "32.  anarchist\n",
      "33.  institutions\n",
      "34.  advocates\n",
      "35.  from\n",
      "36.  an\n",
      "37.  harmful\n",
      "38.  political\n",
      "39.  mutual\n",
      "40. ge\n",
      "41. ier\n",
      "42.  (\n",
      "43.  unjust\n",
      "44.  philosophy\n",
      "45. arch\n",
      "46.  with\n",
      "47. managed\n",
      "48.  anarchy\n",
      "49.  other\n",
      "50.  specifically\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(input_text, transformer, tokenizer, top_k=5, max_length=128):\n",
    "    input_tokens_full = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "    if input_tokens_full.shape[1] > max_length:\n",
    "        input_tokens = input_tokens_full[:, -max_length:]\n",
    "    else:\n",
    "        input_tokens = input_tokens_full\n",
    "    seq_len = input_tokens.shape[1]\n",
    "    logits = transformer(input_tokens, training=False)\n",
    "    logits = logits[0, :]  # Get the logits for the last token\n",
    "    print(logits)\n",
    "    top_k_indices = tf.math.top_k(logits, k=top_k).indices\n",
    "    print(top_k_indices.numpy())\n",
    "    top_k_tokens = [tokenizer.decode([token_id]) for token_id in top_k_indices.numpy()]\n",
    "    \n",
    "    return top_k_tokens\n",
    "\n",
    "\n",
    "input_text = \"\"\"Anarchism was in 1912,\"\"\"\n",
    "predicted_words = predict_next_word(input_text, transformer, tokenizer, top_k=50)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(\"Predicted next words:\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(f\"{i + 1}. {word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
