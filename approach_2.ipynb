{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Layer, Dense, Dropout, Embedding\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = tf.range(position, dtype=tf.float32)[:, tf.newaxis] * 1 / tf.pow(10000, (2 * tf.range(0, d_model, dtype=tf.float32)) / d_model)\n",
    "    angle_rads_even = tf.math.sin(angle_rads[:, 0::2])\n",
    "    angle_rads_odd = tf.math.cos(angle_rads[:, 1::2])\n",
    "    angle_rads = tf.reshape(tf.concat([angle_rads_even, angle_rads_odd], axis=-1), (-1, d_model))\n",
    "    pos_encoding = angle_rads[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class GPTLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.15):\n",
    "        super(GPTLayer, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.mha._build_from_signature(input_shape, input_shape, input_shape)\n",
    "        super(GPTLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask[tf.newaxis, tf.newaxis, :, :] \n",
    "\n",
    "\n",
    "class GPT(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_position_encoding, rate=0.15):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_position_encoding, d_model)\n",
    "\n",
    "        self.gpt_layers = [GPTLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "        self.final_layer = Dense(vocab_size)\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(inp)[1])\n",
    "        padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)\n",
    "        padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "        combined_mask: tf.Tensor = tf.maximum(look_ahead_mask, padding_mask)\n",
    "        return combined_mask\n",
    "\n",
    "\n",
    "    def call(self, x, training):\n",
    "            seq_len = tf.shape(x)[1]\n",
    "            mask = self.create_masks(x)\n",
    "\n",
    "            x = self.embedding(x)\n",
    "            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "            x = self.dropout(x, training=training)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.gpt_layers[i](x, training, mask)\n",
    "\n",
    "            final_output = self.final_layer(x)\n",
    "            last_position_logits = final_output[:, -1, :]\n",
    "            return last_position_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"Connecting to TPU...\")\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='node-7',zone='us-central1-f')\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "print(\"Done!\")\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer: GPT2Tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "with strategy.scope():\n",
    "    # specify parameters here\n",
    "    num_layers = 2\n",
    "    d_model = 300\n",
    "    num_heads = 2\n",
    "    dff = 1200\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    max_position_encoding = 20\n",
    "    dropout_rate = 0.15\n",
    "    batch_size = 55\n",
    "    epochs = 200\n",
    "    warmup_steps = 100\n",
    "\n",
    "    print('Creating model...')\n",
    "    transformer = GPT(num_layers, d_model, num_heads, dff, vocab_size, max_position_encoding, dropout_rate)\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        loss_ = loss_object(real, pred)\n",
    "        return loss_\n",
    "\n",
    "    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super(CustomSchedule, self).__init__()\n",
    "\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            learning_rate = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "            return learning_rate\n",
    "\n",
    "    learning_rate = CustomSchedule(d_model, warmup_steps)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "bucket_path = 'gs://dataset_w/'\n",
    "input_tfrecord_files = [f'{bucket_path}wikitrain_{i:04d}.tfrecord' for i in range(79)]\n",
    "\n",
    "def create_tf_dataset(data, tokenizer):\n",
    "    def split_input_target(input_string):\n",
    "        parts = input_string.strip().split(\"? \")\n",
    "        event, year = \" \".join(parts[:-1]), int(parts[-1])\n",
    "        return event, year\n",
    "\n",
    "    events, years = zip(*[split_input_target(item) for item in data])\n",
    "    \n",
    "    # Encode events using GPT-2 tokenizer\n",
    "    encoded_events = [tokenizer.encode(event) for event in events]\n",
    "    \n",
    "    events_max_length = max([len(event) for event in encoded_events])\n",
    "    \n",
    "    encoded_events = [[0] * (events_max_length - len(event)) + event for event in encoded_events]\n",
    "    \n",
    "    encoded_years = tf.expand_dims(years, axis=1)\n",
    "    events_tensor = tf.data.Dataset.from_tensor_slices(encoded_events)\n",
    "    years_tensor = tf.data.Dataset.from_tensor_slices(encoded_years)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((events_tensor, years_tensor))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "data = [\"What year was the signing of the Declaration of Independence? 1776\",\n",
    "\"What year was the storming of the Bastille? 1789\",\n",
    "\"What year was the Battle of Waterloo? 1815\",\n",
    "\"What year was the assassination of Abraham Lincoln? 1865\",\n",
    "\"What year was the invention of the telephone by Alexander Graham Bell? 1876\",\n",
    "\"What year was the first successful powered airplane flight by the Wright brothers? 1903\",\n",
    "\"What year was the sinking of the Titanic? 1912\",\n",
    "\"What year was the beginning of World War I? 1914\",\n",
    "\"What year was the Russian Revolution? 1917\",\n",
    "\"What year was the end of World War I? 1918\",\n",
    "\"What year was the stock market crash that led to the Great Depression? 1929\",\n",
    "\"What year was the beginning of World War II? 1939\",\n",
    "\"What year was the attack on Pearl Harbor? 1941\",\n",
    "\"What year was the D-Day invasion during World War II? 1944\",\n",
    "\"What year was the dropping of the atomic bombs on Hiroshima and Nagasaki? 1945\",\n",
    "\"What year was the end of World War II? 1945\",\n",
    "\"What year was the establishment of the United Nations? 1945\",\n",
    "\"What year was the beginning of the Korean War? 1950\",\n",
    "\"What year was the launch of Sputnik 1, the first artificial satellite? 1957\",\n",
    "\"What year was the Cuban Missile Crisis? 1962\",\n",
    "\"What year was the assassination of John F. Kennedy? 1963\",\n",
    "\"What year was the first moon landing by Apollo 11? 1969\",\n",
    "\"What year was the end of the Vietnam War? 1975\",\n",
    "\"What year was the fall of the Berlin Wall? 1989\",\n",
    "\"What year was the dissolution of the Soviet Union? 1991\",\n",
    "\"What year was the terrorist attacks on September 11? 2001\",\n",
    "\"What year was the beginning of the Iraq War? 2003\",\n",
    "\"What year was the invention of the World Wide Web by Tim Berners-Lee? 1989\",\n",
    "\"What year was the assassination of Martin Luther King Jr.? 1968\",\n",
    "\"What year was the discovery of DNA's double helix structure by James Watson and Francis Crick? 1953\",\n",
    "\"What year was the first human heart transplant performed by Dr. Christiaan Barnard? 1967\",\n",
    "\"What year was the Chernobyl nuclear disaster? 1986\",\n",
    "\"What year was the launch of the Hubble Space Telescope? 1990\",\n",
    "\"What year was the Rwandan Genocide? 1994\",\n",
    "\"What year was the Oklahoma City bombing? 1995\",\n",
    "\"What year was the cloning of Dolly the sheep? 1996\",\n",
    "\"What year was the death of Princess Diana? 1997\",\n",
    "\"What year was the Euro currency introduced? 1999\",\n",
    "\"What year was the Indian Ocean earthquake and tsunami? 2004\",\n",
    "\"What year was the election of Pope Francis? 2013\",\n",
    "\"What year was the Paris Agreement on climate change signed? 2016\",\n",
    "\"What year was the Brexit referendum? 2016\",\n",
    "\"What year was the first iPhone released? 2007\",\n",
    "\"What year was the election of Donald Trump as the 45th President of the United States? 2016\",\n",
    "\"What year was the completion of the Human Genome Project? 2003\",\n",
    "\"What year was the founding of the World Health Organization? 1948\",\n",
    "\"What year was the assassination of Archduke Franz Ferdinand? 1914\",\n",
    "\"What year was the start of the California Gold Rush? 1848\",\n",
    "\"What year was the completion of the Panama Canal? 1914\",\n",
    "\"What year was the discovery of penicillin by Alexander Fleming? 1928\",\n",
    "\"What year was the Montgomery Bus Boycott? 1955\",\n",
    "\"What year was the assassination of Mahatma Gandhi? 1948\",\n",
    "\"What year was the formation of the European Union? 1993\",\n",
    "\"What year was the release of the first Harry Potter book by J.K. Rowling? 1997\",\n",
    "\"What year was the start of the American Civil War? 1861\"]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "dataset = create_tf_dataset(data, tokenizer)\n",
    "\n",
    "def print_sequences_as_words(inp, tar):\n",
    "    inp_tokens = tokenizer.batch_decode(inp.numpy(), skip_special_tokens=True)\n",
    "    tar_tokens = tokenizer.batch_decode([tar.numpy()], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Input:\")\n",
    "    for seq in inp_tokens:\n",
    "        print(seq)\n",
    "\n",
    "    print(\"\\nTarget:\")\n",
    "    for seq in tar_tokens:\n",
    "        print(seq)\n",
    "def print_dataset(input_dataset, num_examples=1):\n",
    "    for i, (inp, tar) in enumerate(input_dataset.take(num_examples)):\n",
    "        print(f\"Example {i + 1}:\")\n",
    "        print(\"Input: \", inp.numpy())\n",
    "        print(\"Target: \", tar.numpy())\n",
    "        print_sequences_as_words(inp, tar)\n",
    "        print(\"\\n\")\n",
    "# print_dataset(dataset, num_examples=3)\n",
    "\n",
    "dataset = dataset.shuffle(50)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "\n",
    "def plot_loss(loss_history):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    plt.title(f\"Loss History\\nLast Updated at: {now}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(inp, training=True)\n",
    "        loss = loss_function(tar, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "loss_history = []\n",
    "print(\"Initializing training...\")\n",
    "for epoch in range(epochs):\n",
    "    total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        per_replica_losses = strategy.run(train_step, args=(inp, tar))\n",
    "        loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "        total_loss += loss\n",
    "        loss = loss.numpy()\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        plot_loss(loss_history)\n",
    "\n",
    "    avg_loss = total_loss / (batch + 1)\n",
    "    print(f'Epoch {epoch + 1}, Average loss: {avg_loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(loss_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, transformer, tokenizer, top_k=5, max_length=128):\n",
    "    input_text = input_text.replace(\"?\", \"\")\n",
    "    input_tokens_full = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "    if input_tokens_full.shape[1] > max_length:\n",
    "        input_tokens = input_tokens_full[:, -max_length:]\n",
    "    elif input_tokens_full.shape[1] < max_length:\n",
    "        input_tokens = tf.pad(input_tokens_full, [[0, 0], [max_length - input_tokens_full.shape[1], 0]])\n",
    "    print(input_tokens)\n",
    "    logits = transformer(input_tokens, training=False)\n",
    "    logits = logits[0, :]\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    top_k_indices = tf.math.top_k(probabilities, k=top_k).indices\n",
    "    return top_k_indices\n",
    "\n",
    "\n",
    "input_text = \"\"\"When did the US declare independence?\"\"\"\n",
    "predicted_words = predict_next_word(input_text, transformer, tokenizer, top_k=10, max_length=max_position_encoding)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(\"Predicted next words:\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(f\"{i + 1}. {word}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Anarchism was in 1912,\n",
    "Predicted next words:\n",
    "1. ,\n",
    "2.  and\n",
    "3.  the\n",
    "4. .\n",
    "5.  as\n",
    "6. ism\n",
    "7.  of\n",
    "8.  to\n",
    "9.  anarchism\n",
    "10.  These\n",
    "11.  a\n",
    "12.  or\n",
    "13. -\n",
    "14.  on\n",
    "15.  often\n",
    "16. 's\n",
    "17.  communism\n",
    "18.  self\n",
    "19.  voluntary\n",
    "20.  free\n",
    "21.  not\n",
    "22.  them\n",
    "23.  been\n",
    "24.  based\n",
    "25.  cooperative\n",
    "26. ed\n",
    "27.  it\n",
    "28.  far\n",
    "29.  central\n",
    "30.  described\n",
    "31.  is\n",
    "32.  anarchist\n",
    "33.  institutions\n",
    "34.  advocates\n",
    "35.  from\n",
    "...\n",
    "47. managed\n",
    "48.  anarchy\n",
    "49.  other\n",
    "50.  specifically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
