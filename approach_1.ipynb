{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Layer, Dense, Dropout, Embedding\n",
    "from tensorflow.python.keras.models import Model\n",
    "# from tensorflow.python.keras import regularizers\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = tf.range(position, dtype=tf.float32)[:, tf.newaxis] * 1 / tf.pow(10000, (2 * tf.range(0, d_model, dtype=tf.float32)) / d_model)\n",
    "    angle_rads_even = tf.math.sin(angle_rads[:, 0::2])\n",
    "    angle_rads_odd = tf.math.cos(angle_rads[:, 1::2])\n",
    "    angle_rads = tf.reshape(tf.concat([angle_rads_even, angle_rads_odd], axis=-1), (-1, d_model))\n",
    "    pos_encoding = angle_rads[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class GPTLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.15):\n",
    "        super(GPTLayer, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.mha._build_from_signature(input_shape, input_shape, input_shape)\n",
    "        super(GPTLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "class GPT(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_position_encoding, rate=0.15):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_position_encoding, d_model)\n",
    "\n",
    "        self.gpt_layers = [GPTLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "        self.final_layer = Dense(vocab_size)\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(inp)[1])\n",
    "        padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)\n",
    "        padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "        combined_mask: tf.Tensor = tf.maximum(look_ahead_mask, padding_mask)\n",
    "        return combined_mask\n",
    "\n",
    "\n",
    "    def call(self, x, training):\n",
    "            seq_len = tf.shape(x)[1]\n",
    "            mask = self.create_masks(x)\n",
    "\n",
    "            x = self.embedding(x)\n",
    "            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "            x = self.dropout(x, training=training)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.gpt_layers[i](x, training, mask)\n",
    "\n",
    "            final_output = self.final_layer(x)\n",
    "            last_position_logits = final_output[:, -1, :]  # Get logits for last pos\n",
    "            return last_position_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Embedding, Layer\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"Connecting to TPU...\")\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='node-7',zone='us-central1-f')\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "print(\"Done!\")\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer: GPT2Tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "with strategy.scope():\n",
    "    # specify parameters here\n",
    "    num_layers = 2\n",
    "    d_model = 300\n",
    "    num_heads = 2\n",
    "    dff = 1200\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    max_position_encoding = 20\n",
    "    dropout_rate = 0.15\n",
    "    batch_size = 1024\n",
    "    epochs = 3\n",
    "    warmup_steps = 4000\n",
    "\n",
    "    print('Creating model...')\n",
    "    transformer = GPT(num_layers, d_model, num_heads, dff, vocab_size, max_position_encoding, dropout_rate)\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        loss_ = loss_object(real, pred)\n",
    "        return loss_\n",
    "\n",
    "\n",
    "    \n",
    "    class NoamSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super(NoamSchedule, self).__init__()\n",
    "\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            learning_rate = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "            return learning_rate\n",
    "\n",
    "    learning_rate = NoamSchedule(d_model, warmup_steps)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "bucket_path = 'gs://dataset_w/'\n",
    "input_tfrecord_files = [f'{bucket_path}wikitrain_{i:04d}.tfrecord' for i in range(79)]\n",
    "\n",
    "def create_windows(sequence, step=1):\n",
    "    sequence_length = tf.shape(sequence)[0]\n",
    "\n",
    "    num_windows = sequence_length - step\n",
    "\n",
    "    windows = tf.TensorArray(dtype=tf.int64, size=num_windows, dynamic_size=True)\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        windows = windows.write(i, sequence[i:i + step + 1])\n",
    "\n",
    "    return windows.stack()\n",
    "\n",
    "@tf.function\n",
    "def _parse_function(example_proto):\n",
    "    feature_description = {\n",
    "        'token_ids': tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    input_sequence = tf.sparse.to_dense(parsed_features['token_ids'])\n",
    "    input_sequences = create_windows(input_sequence, max_position_encoding)  # Add 1 to window size for target\n",
    "\n",
    "    def process_sequences(seq):\n",
    "        inp = seq[:-1]\n",
    "        tar = seq[-1]\n",
    "        return inp, tar\n",
    "\n",
    "    input_sequences, target_sequences = tf.map_fn(process_sequences, input_sequences, dtype=(tf.int64, tf.int64))\n",
    "\n",
    "    return input_sequences, target_sequences\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_dataset(input_files):\n",
    "    input_ds = tf.data.TFRecordDataset(input_files)\n",
    "    input_ds = input_ds.map(_parse_function)\n",
    "    return input_ds\n",
    "\n",
    "\n",
    "print('Processing dataset...')\n",
    "input_dataset = load_dataset(input_tfrecord_files)\n",
    "\n",
    "def print_sequences_as_words(inp, tar):\n",
    "    inp_tokens = tokenizer.batch_decode(inp.numpy(), skip_special_tokens=True)\n",
    "    tar_tokens = tokenizer.batch_decode([tar.numpy()], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Input:\")\n",
    "    for seq in inp_tokens:\n",
    "        print(seq)\n",
    "\n",
    "    print(\"\\nTarget:\")\n",
    "    for seq in tar_tokens:\n",
    "        print(seq)\n",
    "def print_dataset(input_dataset, num_examples=1):\n",
    "    for i, (inp, tar) in enumerate(input_dataset.take(num_examples)):\n",
    "        print(f\"Example {i + 1}:\")\n",
    "        print(\"Input: \", inp.numpy())\n",
    "        print(\"Target: \", tar.numpy())\n",
    "        print_sequences_as_words(inp, tar)\n",
    "        print(\"\\n\")\n",
    "# print_dataset(input_dataset, 1)\n",
    "\n",
    "input_dataset = input_dataset.flat_map(lambda x, y: tf.data.Dataset.from_tensor_slices((x, y)))\n",
    "dataset = input_dataset.batch(batch_size, drop_remainder=True)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print('Done!')\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_loss(loss_history):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    plt.title(f\"Loss History\\nLast Updated at: {now}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(inp, training=True)\n",
    "        loss = loss_function(tar, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "loss_history = []\n",
    "print(\"Initializing training...\")\n",
    "for epoch in range(epochs):\n",
    "    total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        per_replica_losses = strategy.run(train_step, args=(inp, tar))\n",
    "        loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "        total_loss += loss\n",
    "        loss = loss.numpy()\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        \n",
    "    avg_loss = total_loss / (batch + 1)\n",
    "    print(f'Epoch {epoch + 1}, Average loss: {avg_loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w(warm, d_model, step): \n",
    "    arg1 = step ** -0.5\n",
    "    arg2 = step * (warm ** -1.5)\n",
    "    learning_rate = d_model ** 0.5  * min(arg1, arg2)\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "learning_rate = []\n",
    "d_model = 512\n",
    "warm = 4000\n",
    "learning_rate = [w(warm, d_model, step) for step in range(1, 100000)]\n",
    "\n",
    "plt.plot(learning_rate)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "\n",
    "# Get the current timestamp\n",
    "plt.title(f\"Learning rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, transformer, tokenizer, top_k=5, max_length=128):\n",
    "    input_tokens_full = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "    if input_tokens_full.shape[1] > max_length:\n",
    "        input_tokens = input_tokens_full[:, -max_length:]\n",
    "    elif input_tokens_full.shape[1] < max_length:\n",
    "        input_tokens = tf.pad(input_tokens_full, [[0, 0], [max_length - input_tokens_full.shape[1], 0]])\n",
    "    print(input_tokens)\n",
    "    logits = transformer(input_tokens, training=False)\n",
    "    logits = logits[0, :]  # Get the logits for the last token\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    top_k_indices = tf.math.top_k(probabilities, k=top_k).indices\n",
    "    top_k_tokens = [tokenizer.decode([token_id]) for token_id in top_k_indices.numpy()]\n",
    "    \n",
    "    return top_k_tokens\n",
    "\n",
    "\n",
    "input_text = \"\"\"The first, the first,\"\"\"\n",
    "predicted_words = predict_next_word(input_text, transformer, tokenizer, top_k=50, max_length=max_position_encoding)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(\"Predicted next words:\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(f\"{i + 1}. {word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
